{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ae9a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print: True\n",
    "print(torch.cuda.get_device_name(0))  # Should print: Tesla T4\n",
    "!pip install torch torchvision torchaudio transformers rasterio geopandas streamlit pyngrok pandas numpy matplotlib scikit-learn opencv-python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from sklearn.metrics import accuracy_score, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d78e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Define Dataset Classes\n",
    "class KaggleWildfireDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['nowildfire', 'wildfire']\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for cls_idx, cls in enumerate(self.classes):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if os.path.exists(cls_dir):\n",
    "                for img_name in os.listdir(cls_dir):\n",
    "                    img_path = os.path.join(cls_dir, img_name)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            img.load()\n",
    "                            self.images.append(img_path)\n",
    "                            self.labels.append(cls_idx)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping corrupted file: {img_path} - {e}\")\n",
    "            else:\n",
    "                print(f\"Directory not found: {cls_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class EuroSATDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for cls_idx, cls in enumerate(self.classes):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_dir):\n",
    "                for img_name in os.listdir(cls_dir):\n",
    "                    self.images.append(os.path.join(cls_dir, img_name))\n",
    "                    self.labels.append(cls_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class DeepGlobeDataset(Dataset):\n",
    "    def __init__(self, metadata_file, class_dict_file, root_dir, transform=None, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        # Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_file)\n",
    "        self.metadata = self.metadata[self.metadata['split'] == split]\n",
    "        self.image_paths = self.metadata['sat_image_path'].apply(lambda x: os.path.join(root_dir, x)).tolist()\n",
    "        self.mask_paths = self.metadata['mask_path'].apply(lambda x: os.path.join(root_dir, x) if pd.notna(x) else None).tolist()\n",
    "        # Load class dictionary\n",
    "        self.class_dict = pd.read_csv(class_dict_file)\n",
    "        self.class_to_idx = {row['name']: idx for idx, row in self.class_dict.iterrows()}\n",
    "        self.color_to_class = {tuple(row[['r', 'g', 'b']].values): idx for idx, row in self.class_dict.iterrows()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.mask_paths[idx] is not None and os.path.exists(self.mask_paths[idx]):\n",
    "            mask = Image.open(self.mask_paths[idx])\n",
    "            mask = np.array(mask)\n",
    "            # Convert RGB mask to class indices\n",
    "            mask_converted = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
    "            for color, class_idx in self.color_to_class.items():\n",
    "                mask_converted[np.all(mask == color, axis=2)] = class_idx\n",
    "            # Resize mask to match image\n",
    "            mask_image = Image.fromarray(mask_converted)\n",
    "            mask_image = mask_image.resize((512, 512), Image.NEAREST)  # Use NEAREST for segmentation masks\n",
    "            mask_converted = np.array(mask_image)\n",
    "        else:\n",
    "            mask_converted = np.zeros((512, 512), dtype=np.uint8)  # Match image size\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask_converted = torch.tensor(mask_converted, dtype=torch.long)\n",
    "\n",
    "        return image, mask_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb99fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define MSSM-SCDNet Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MSSM_SCDNet(nn.Module):\n",
    "    def __init__(self, num_classes=7, freeze_backbone=False):  # 7 classes from DeepGlobe\n",
    "        super(MSSM_SCDNet, self).__init__()\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for param in self.segformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.cam = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.sam = nn.Sequential(\n",
    "            nn.Conv2d(num_classes, num_classes // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_classes // 2, num_classes, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] != 3:\n",
    "            raise ValueError(f\"Expected 3 input channels, got {x.shape[1]}\")\n",
    "        outputs = self.segformer(x)\n",
    "        logits = outputs.logits\n",
    "        logits = nn.functional.interpolate(logits, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam_out = self.cam(logits)\n",
    "        sam_out = self.sam(logits)\n",
    "        logits = logits * sam_out\n",
    "        return logits\n",
    "\n",
    "# Initialize model (unfrozen for full retraining)\n",
    "segmenter = MSSM_SCDNet(num_classes=7, freeze_backbone=False).to(device)\n",
    "print(\"Model initialized with 7 classes, full retraining recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2ddc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define Classification Model (Kaggle Wildfire)\n",
    "class WildfireClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(WildfireClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 87 * 87, 256)  # Adjusted for 350x350 resize\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdd3e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Define Transforms\n",
    "deepglobe_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "kaggle_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((350, 350)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eurosat_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b31cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Load Datasets\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "data_root = '/content/drive/MyDrive/WildfireProject/datasets/'\n",
    "\n",
    "# Kaggle Wildfire\n",
    "kaggle_train = KaggleWildfireDataset(os.path.join(data_root, 'wildfire_prediction/train'), transform=kaggle_transform)\n",
    "kaggle_val = KaggleWildfireDataset(os.path.join(data_root, 'wildfire_prediction/valid'), transform=kaggle_transform)\n",
    "kaggle_test = KaggleWildfireDataset(os.path.join(data_root, 'wildfire_prediction/test'), transform=kaggle_transform)\n",
    "kaggle_train_loader = DataLoader(kaggle_train, batch_size=4, shuffle=True)\n",
    "kaggle_val_loader = DataLoader(kaggle_val, batch_size=4, shuffle=False)\n",
    "kaggle_test_loader = DataLoader(kaggle_test, batch_size=4, shuffle=False)\n",
    "\n",
    "# EuroSAT\n",
    "eurosat_root = os.path.join(data_root, 'eurosat')\n",
    "eurosat_dataset = EuroSATDataset(eurosat_root, transform=eurosat_transform)\n",
    "train_size = int(0.7 * len(eurosat_dataset))\n",
    "val_size = int(0.15 * len(eurosat_dataset))\n",
    "test_size = len(eurosat_dataset) - train_size - val_size\n",
    "eurosat_train, eurosat_val, eurosat_test = torch.utils.data.random_split(\n",
    "    eurosat_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "eurosat_train_loader = DataLoader(eurosat_train, batch_size=4, shuffle=True)\n",
    "eurosat_val_loader = DataLoader(eurosat_val, batch_size=4, shuffle=False)\n",
    "eurosat_test_loader = DataLoader(eurosat_test, batch_size=4, shuffle=False)\n",
    "\n",
    "# DeepGlobe\n",
    "deepglobe_train = DeepGlobeDataset(\n",
    "    metadata_file=os.path.join(data_root, 'deepglobe/metadata.csv'),\n",
    "    class_dict_file=os.path.join(data_root, 'deepglobe/class_dict.csv'),\n",
    "    root_dir=os.path.join(data_root, 'deepglobe'),\n",
    "    transform=deepglobe_transform,\n",
    "    split='train'\n",
    ")\n",
    "deepglobe_val = DeepGlobeDataset(\n",
    "    metadata_file=os.path.join(data_root, 'deepglobe/metadata.csv'),\n",
    "    class_dict_file=os.path.join(data_root, 'deepglobe/class_dict.csv'),\n",
    "    root_dir=os.path.join(data_root, 'deepglobe'),\n",
    "    transform=deepglobe_transform,\n",
    "    split='valid'\n",
    ")\n",
    "deepglobe_test = DeepGlobeDataset(\n",
    "    metadata_file=os.path.join(data_root, 'deepglobe/metadata.csv'),\n",
    "    class_dict_file=os.path.join(data_root, 'deepglobe/class_dict.csv'),\n",
    "    root_dir=os.path.join(data_root, 'deepglobe'),\n",
    "    transform=deepglobe_transform,\n",
    "    split='test'\n",
    ")\n",
    "deepglobe_train_loader = DataLoader(deepglobe_train, batch_size=2, shuffle=True)  # Increased to 2\n",
    "deepglobe_val_loader = DataLoader(deepglobe_val, batch_size=2, shuffle=False)\n",
    "deepglobe_test_loader = DataLoader(deepglobe_test, batch_size=2, shuffle=False)\n",
    "\n",
    "# Verify dataset sizes\n",
    "print(f\"DeepGlobe Train: {len(deepglobe_train)}, Val: {len(deepglobe_val)}, Test: {len(deepglobe_test)}\")\n",
    "print(f\"EuroSAT Train: {len(eurosat_train)}, Val: {len(eurosat_val)}, Test: {len(eurosat_test)}\")\n",
    "print(f\"Kaggle Train: {len(kaggle_train)}, Val: {len(kaggle_val)}, Test: {len(kaggle_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965248d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Initialize Models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Verify GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Wildfire Classifier\n",
    "classifier = WildfireClassifier(num_classes=2).to(device)\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n",
    "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Segmentation Model\n",
    "segmenter = MSSM_SCDNet(num_classes=7).to(device)\n",
    "segmenter_criterion = nn.CrossEntropyLoss()\n",
    "segmenter_optimizer = torch.optim.Adam(segmenter.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f1cf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Train Model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# Training setup (using segmenter from Cell 7 and loader from Cell 6)\n",
    "optimizer = torch.optim.Adam(segmenter.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # Handle potential ignore index in masks\n",
    "\n",
    "num_epochs = 5  # 5 epochs as requested\n",
    "segmenter.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, masks) in enumerate(deepglobe_train_loader):  # Use your existing loader\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = segmenter(images)\n",
    "        # Resize masks to match output (assuming 512x512 from transform)\n",
    "        masks = F.interpolate(masks.unsqueeze(1).float(), size=(512, 512), mode='nearest').squeeze(1).long()\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {running_loss/len(deepglobe_train_loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "os.makedirs('/content/drive/MyDrive/WildfireProject/models', exist_ok=True)\n",
    "torch.save(segmenter.state_dict(), '/content/drive/MyDrive/WildfireProject/models/segmenter_retrained.pth')\n",
    "print(\"Model saved to /content/drive/MyDrive/WildfireProject/models/segmenter_retrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa15ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Train Segmenter (DeepGlobe)\n",
    "def train_segmenter(model, train_loader, val_loader, criterion, optimizer, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f'Segmenter Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, masks).item()\n",
    "        print(f'Segmenter Epoch {epoch+1}, Val Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "print(\"Training Segmenter (DeepGlobe)...\")\n",
    "train_segmenter(segmenter, deepglobe_train_loader, deepglobe_val_loader, segmenter_criterion, segmenter_optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2fa30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate Models\n",
    "def evaluate_classifier(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            ground_truths.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "    print(f'Classifier Test Accuracy: {accuracy}')\n",
    "    return predictions, ground_truths\n",
    "\n",
    "def evaluate_segmenter(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "    print('Segmenter Test Inference Done (No mIoU due to missing test masks)')\n",
    "    return predictions\n",
    "\n",
    "print(\"Evaluating Classifier...\")\n",
    "evaluate_classifier(classifier, kaggle_test_loader)\n",
    "\n",
    "print(\"Evaluating Segmenter...\")\n",
    "evaluate_segmenter(segmenter, deepglobe_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992be74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Generate Change Map\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_change_map(segmenter, image_path):  # Removed classifier dependency\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    print(f\"Input shape: {image_tensor.shape}\")\n",
    "\n",
    "    # Segmenter inference (skip classifier for now)\n",
    "    segmenter.eval()\n",
    "    with torch.no_grad():\n",
    "        seg_output = segmenter(image_tensor)\n",
    "        print(f\"Segmenter output shape: {seg_output.shape}\")\n",
    "        seg_pred = torch.argmax(seg_output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Save change map\n",
    "    output_path = '/content/drive/MyDrive/WildfireProject/outputs/change_map.png'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    Image.fromarray((seg_pred * 255 / 6).astype(np.uint8)).save(output_path)\n",
    "    print(f\"Change map saved to {output_path}\")\n",
    "\n",
    "# Load retrained segmenter\n",
    "segmenter = MSSM_SCDNet(num_classes=7).to(device)\n",
    "segmenter.load_state_dict(torch.load('/content/drive/MyDrive/WildfireProject/models/segmenter_retrained.pth'))\n",
    "segmenter.eval()\n",
    "\n",
    "# Run\n",
    "test_image_path = os.path.join(data_root, 'deepglobe/test/100877_sat.jpg')\n",
    "generate_change_map(segmenter, test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66d664",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 12: Compute NDVI (Placeholder)\n",
    "def compute_ndvi(image_path):\n",
    "    print(\"NDVI requires multi-spectral bands (NIR, Red). EuroSAT is RGB-only.\")\n",
    "    # Future: Use Sentinel-2 data for NDVI\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe597f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13: Save Models\n",
    "os.makedirs('/content/drive/MyDrive/WildfireProject/models', exist_ok=True)\n",
    "torch.save(classifier.state_dict(), '/content/drive/MyDrive/WildfireProject/models/classifier.pth')\n",
    "torch.save(segmenter.state_dict(), '/content/drive/MyDrive/WildfireProject/models/segmenter_retrained.pth')\n",
    "print(\"Models saved to /content/drive/MyDrive/WildfireProject/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"/content/drive/MyDrive/WildfireProject/datasets/deepglobe/\"\n",
    "model_path = \"/content/drive/MyDrive/WildfireProject/models/segmenter_retrained.pth\"\n",
    "metadata_csv = \"/content/drive/MyDrive/WildfireProject/datasets/deepglobe/metadata.csv\"\n",
    "class_dict_csv = \"/content/drive/MyDrive/WildfireProject/datasets/deepglobe/class_dict.csv\"\n",
    "\n",
    "# Define the model (same as in your app)\n",
    "class MSSM_SCDNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(MSSM_SCDNet, self).__init__()\n",
    "        from transformers import SegformerForSemanticSegmentation\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.cam = torch.nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
    "        self.sam = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(num_classes, num_classes // 2, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(num_classes // 2, num_classes, kernel_size=3, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.segformer(x)\n",
    "        logits = outputs.logits\n",
    "        logits = torch.nn.functional.interpolate(logits, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam_out = self.cam(logits)\n",
    "        sam_out = self.sam(logits)\n",
    "        logits = logits * sam_out\n",
    "        return logits\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segmenter = MSSM_SCDNet(num_classes=7).to(device)\n",
    "segmenter.load_state_dict(torch.load(model_path, map_location=device))\n",
    "segmenter.eval()\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load metadata CSV and filter for train split\n",
    "metadata = pd.read_csv(metadata_csv)\n",
    "train_metadata = metadata[metadata['split'] == 'train']\n",
    "\n",
    "# Filter for rows with valid string mask paths\n",
    "train_metadata = train_metadata[train_metadata['mask_path'].apply(lambda x: isinstance(x, str))]\n",
    "image_paths = train_metadata['sat_image_path'].apply(lambda x: os.path.join(base_dir, x)).tolist()\n",
    "mask_paths = train_metadata['mask_path'].apply(lambda x: os.path.join(base_dir, x)).tolist()\n",
    "\n",
    "# Load class_dict.csv for class mapping\n",
    "class_dict = pd.read_csv(class_dict_csv)\n",
    "class_rgb = {i: (r, g, b) for i, (name, r, g, b) in enumerate(class_dict.values)}  # Map index to RGB\n",
    "class_names = class_dict['name'].tolist()\n",
    "\n",
    "def rgb_to_class(mask):\n",
    "    \"\"\"Convert RGB mask to class indices based on class_dict.csv.\"\"\"\n",
    "    mask = np.array(mask.convert('RGB'))  # Ensure RGB mode\n",
    "    h, w, _ = mask.shape\n",
    "    class_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    for class_idx, (r, g, b) in class_rgb.items():\n",
    "        class_map[(mask[:, :, 0] == r) & (mask[:, :, 1] == g) & (mask[:, :, 2] == b)] = class_idx\n",
    "    return class_map\n",
    "\n",
    "# Collect predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for img_path, mask_path in zip(image_paths[:10], mask_paths[:10]):  # Limit to 10 for speed\n",
    "    # Load image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        seg_output = segmenter(image_tensor)\n",
    "        pred = torch.argmax(seg_output, dim=1).squeeze().cpu().numpy()\n",
    "        pred = np.clip(pred, 0, 6)  # Ensure predictions are within 0-6\n",
    "\n",
    "    # Load and convert ground truth mask\n",
    "    try:\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = rgb_to_class(mask)  # Convert RGB to class indices\n",
    "        # Convert mask to PIL Image for resizing\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_resized = mask_pil.resize((512, 512), Image.NEAREST)  # Resize using PIL\n",
    "        mask = np.array(mask_resized)  # Convert back to NumPy\n",
    "        mask = np.clip(mask, 0, 6)  # Ensure masks are within 0-6\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Mask not found at {mask_path}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(mask)\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Debugging output\n",
    "print(\"Shape of all_preds:\", all_preds.shape)\n",
    "print(\"Unique values in all_preds:\", np.unique(all_preds))\n",
    "print(\"Shape of all_labels:\", all_labels.shape)\n",
    "print(\"Unique values in all_labels:\", np.unique(all_labels))\n",
    "\n",
    "# Calculate metrics\n",
    "num_classes = 7\n",
    "iou_per_class = np.zeros(num_classes)\n",
    "for c in range(num_classes):\n",
    "    true_pos = np.sum((all_preds == c) & (all_labels == c))\n",
    "    false_pos = np.sum((all_preds == c) & (all_labels != c))\n",
    "    false_neg = np.sum((all_preds != c) & (all_labels == c))\n",
    "    iou = true_pos / (true_pos + false_pos + false_neg) if (true_pos + false_pos + false_neg) > 0 else 0\n",
    "    iou_per_class[c] = iou\n",
    "\n",
    "mIoU = np.mean(iou_per_class)\n",
    "\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "precision = precision_score(all_labels.flatten(), all_preds.flatten(), average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels.flatten(), all_preds.flatten(), average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels.flatten(), all_preds.flatten(), average='weighted', zero_division=0)\n",
    "kappa = cohen_kappa_score(all_labels.flatten(), all_preds.flatten())\n",
    "cm = confusion_matrix(all_labels.flatten(), all_preds.flatten())\n",
    "oa = accuracy  # Overall Accuracy is the same as accuracy for pixel-wise classification\n",
    "\n",
    "# Print metrics for reference\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"mIoU: {mIoU:.2f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n",
    "print(f\"Overall Accuracy (OA): {oa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart for Accuracy, Precision, Recall, F1-score\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics = [accuracy, precision, recall, f1]\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "plt.bar(metric_names, metrics, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.title('Classification Metrics (Training Subset)')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(metrics):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('/content/drive/MyDrive/WildfireProject/outputs/classification_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Bar Chart for Accuracy, Precision, Recall, F1-score\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics = [accuracy, precision, recall, f1]\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "plt.bar(metric_names, metrics, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.title('Classification Metrics (Training Subset)')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(metrics):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('/content/drive/MyDrive/WildfireProject/outputs/classification_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix (Training Subset)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig('/content/drive/MyDrive/WildfireProject/outputs/confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Bar Chart for OA and Kappa\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics_oa_kappa = [oa, kappa]\n",
    "metric_names_oa_kappa = ['Overall Accuracy', \"Cohen's Kappa\"]\n",
    "plt.bar(metric_names_oa_kappa, metrics_oa_kappa, color=['cyan', 'magenta'])\n",
    "plt.title('Overall Accuracy and Kappa (Training Subset)')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(metrics_oa_kappa):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('/content/drive/MyDrive/WildfireProject/outputs/oa_kappa_metrics.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
